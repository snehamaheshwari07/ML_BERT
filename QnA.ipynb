{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Integration in Question-Answering (QA) Systems\n",
        "\n",
        "This notebook assumes the use of Google Colab. For running locally, make sure all relevant packages are installed into your environment"
      ],
      "metadata": {
        "id": "lfxweDP49Grj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install & import necessary packages"
      ],
      "metadata": {
        "id": "BftleBDONqcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jvbJeJk3tCf",
        "outputId": "0512a8ba-7b1c-46db-d73d-8ada431770ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsh98ARg3f4X",
        "outputId": "13e0d08a-7be1-4034-fb31-76614ba74be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove pretraining warnings, if desired"
      ],
      "metadata": {
        "id": "lVHwRHst9vkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import logging\n",
        "logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "ddpO6tAl638p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check GPU/cuda"
      ],
      "metadata": {
        "id": "0-QDIo4E95ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvzran5j7fkm",
        "outputId": "9f2c09ed-2aaa-466b-f258-686fe3686b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define desired save/load path"
      ],
      "metadata": {
        "id": "21gXkti8-A6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = '/content/drive/MyDrive/CS7641'"
      ],
      "metadata": {
        "id": "5SGCVacQonAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that path exists"
      ],
      "metadata": {
        "id": "GvAq8LpS-Ndu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(source_path):\n",
        "    os.mkdir(source_path)"
      ],
      "metadata": {
        "id": "kv_7-CGp30FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get SQuAD Dataset"
      ],
      "metadata": {
        "id": "KMwG0ZBD-TX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q00X1sJ9eFgI",
        "outputId": "8f855911-0fa7-47e3-daf5-7ba712361dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘train-v2.0.json’ already there; not retrieving.\n",
            "\n",
            "File ‘dev-v2.0.json’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seperate contexts, questions, and answers"
      ],
      "metadata": {
        "id": "QQL9g5Cm-nJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "    contexts, questions, answers = [], [], []\n",
        "\n",
        "    for data in dataset['data']:\n",
        "        for paragraph in data['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            for qas in paragraph['qas']:\n",
        "                question = qas['question']\n",
        "                for answer in qas['answers']:\n",
        "                    text = answer['text']\n",
        "                    answer_start = answer['answer_start']\n",
        "                    answer['answer_end'] = answer_start + len(text)\n",
        "\n",
        "                    # # SQuAD labels can be off by an index or two sometimes\n",
        "                    # if context[answer_start:answer_end] == text:\n",
        "                    #     answer['answer_end'] = answer_end\n",
        "                    # elif context[answer_start-1:answer_end-1] == text:\n",
        "                    #     answer['answer_start'] = answer_start - 1\n",
        "                    #     answer['answer_end'] = answer_end - 1\n",
        "                    # elif context[answer_start-2:answer_end-2] == text:\n",
        "                    #     answer['answer_start'] = answer_start - 2\n",
        "                    #     answer['answer_end'] = answer_end - 2\n",
        "\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "\n",
        "    return contexts, questions, answers"
      ],
      "metadata": {
        "id": "djcyJLvIeL8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If loading SQuAD instead of downloading, change paths here\n",
        "train_path, test_path = 'train-v2.0.json', 'dev-v2.0.json'\n",
        "train_contexts, train_questions, train_answers = load_data(train_path)\n",
        "test_contexts, test_questions, test_answers = load_data(test_path)"
      ],
      "metadata": {
        "id": "RILIjZDPfioi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define tokenizer & encode data"
      ],
      "metadata": {
        "id": "SByzxLKn-uPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "hbMAP55n4pHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(train_contexts,\n",
        "                            train_questions,\n",
        "                            truncation=True,\n",
        "                            padding=True)\n",
        "\n",
        "test_encodings = tokenizer(test_contexts,\n",
        "                           test_questions,\n",
        "                           truncation=True,\n",
        "                           padding=True)"
      ],
      "metadata": {
        "id": "fFs3dZmthpby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add token start/end for answers from character start/end"
      ],
      "metadata": {
        "id": "hrJgpAwB-7Rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answers_char_to_token(answers, encodings):\n",
        "    answer_start_tokens, answer_end_tokens = [], []\n",
        "    for i in range(len(answers)):\n",
        "        start_token = encodings.char_to_token(i, answers[i]['answer_start'])\n",
        "        end_token = encodings.char_to_token(i, answers[i]['answer_end'] - 1)\n",
        "\n",
        "        answer_start_tokens.append(start_token)\n",
        "        answer_end_tokens.append(end_token)\n",
        "\n",
        "        # Check for truncated answer passages\n",
        "        if answer_start_tokens[-1] is None:\n",
        "            answer_start_tokens[-1] = tokenizer.model_max_length\n",
        "        if answer_end_tokens[-1] is None:\n",
        "            answer_end_tokens[-1] = tokenizer.model_max_length\n",
        "\n",
        "    encodings.update({'answer_start_tokens': answer_start_tokens,\n",
        "                      'answer_end_tokens': answer_end_tokens})"
      ],
      "metadata": {
        "id": "KW5NLZdDiEWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers_char_to_token(train_answers, train_encodings)\n",
        "answers_char_to_token(test_answers, test_encodings)"
      ],
      "metadata": {
        "id": "_6PjxCyJlDIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Torch Dataset for SQuAD"
      ],
      "metadata": {
        "id": "i3-_1Ikc_EWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        super().__init__()\n",
        "        self.encodings = encodings\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)"
      ],
      "metadata": {
        "id": "bfmRM46BlRrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Custom_Dataset(train_encodings)\n",
        "test_dataset = Custom_Dataset(test_encodings)"
      ],
      "metadata": {
        "id": "TZRfY2ECltR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define hyperparameters"
      ],
      "metadata": {
        "id": "VKOCfk6m_KYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {}\n",
        "hyperparameters['N_EPOCHS'] = 5\n",
        "hyperparameters['learning_rate'] = 5e-5\n",
        "hyperparameters['weight_decay'] = 0.01\n",
        "hyperparameters['batch_size'] = 16"
      ],
      "metadata": {
        "id": "s4ttcsUr0tFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get pretrained base model"
      ],
      "metadata": {
        "id": "i_E3oNeP_b0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "fDTS5BsemHqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "d5JI3T9u_gIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(model, hyperparameters, train_dataset, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f'Using {device}')\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                      lr=hyperparameters['learning_rate'],\n",
        "                      weight_decay=hyperparameters['weight_decay'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              batch_size=hyperparameters['batch_size'],\n",
        "                              shuffle=True)\n",
        "\n",
        "    for epoch in range(hyperparameters['N_EPOCHS']):\n",
        "        step = tqdm(train_loader, leave=True)\n",
        "        for batch in step:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            answer_start_tokens = batch['answer_start_tokens'].to(device)\n",
        "            answer_end_tokens = batch['answer_end_tokens'].to(device)\n",
        "\n",
        "            outputs = model(input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            start_positions=answer_start_tokens,\n",
        "                            end_positions=answer_end_tokens)\n",
        "\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            step.set_description(f'Epoch {epoch+1}')\n",
        "            step.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "id": "UcUeyyB12WHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, hyperparameters, train_dataset, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE-PoQZ04DLE",
        "outputId": "8d311c93-d8e1-4e84-f8e5-ffc80098acc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 5427/5427 [21:28<00:00,  4.21it/s, loss=2.43]\n",
            "Epoch 2: 100%|██████████| 5427/5427 [21:25<00:00,  4.22it/s, loss=1.51]\n",
            "Epoch 3: 100%|██████████| 5427/5427 [21:27<00:00,  4.21it/s, loss=0.04]\n",
            "Epoch 4: 100%|██████████| 5427/5427 [21:25<00:00,  4.22it/s, loss=0.263]\n",
            "Epoch 5: 100%|██████████| 5427/5427 [21:24<00:00,  4.23it/s, loss=0.806]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save trained model, or load model from source path"
      ],
      "metadata": {
        "id": "pnZL-Ger_riP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "def save_model(path, model, tokenizer):\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "\n",
        "def load_model(path, device=None):\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f'Using {device}')\n",
        "\n",
        "    model = model.to(device)\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "0xNqFzvp5kUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment out either save_model or load_model"
      ],
      "metadata": {
        "id": "Rt7RnUdA_0bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(source_path, model, tokenizer)\n",
        "# model, tokenizer = load_model(source_path)"
      ],
      "metadata": {
        "id": "fGN4h8KloeTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test accuracy of model"
      ],
      "metadata": {
        "id": "AdW-vit4__hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def model_accuracy(model, test_dataset, batch_size):\n",
        "    start_accuracies, end_accuracies, batch_accuracies = [], [], []\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader):\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            start_positions_true = batch['answer_start_tokens'].to(device)\n",
        "            end_positions_true = batch['answer_end_tokens'].to(device)\n",
        "\n",
        "            start_positions_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "            end_positions_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
        "\n",
        "            start_accuracy = (start_positions_pred == start_positions_true).sum()\n",
        "            start_accuracy = start_accuracy / len(start_positions_pred)\n",
        "            start_accuracy = start_accuracy.item()\n",
        "            start_accuracies.append(start_accuracy)\n",
        "\n",
        "            end_accuracy = (end_positions_pred == end_positions_true).sum()\n",
        "            end_accuracy = end_accuracy / len(end_positions_pred)\n",
        "            end_accuracy = end_accuracy.item()\n",
        "            end_accuracies.append(end_accuracy)\n",
        "\n",
        "            batch_accuracies.append(start_accuracy)\n",
        "            batch_accuracies.append(end_accuracy)\n",
        "\n",
        "    start_accuracy = sum(start_accuracies) / len(start_accuracies)\n",
        "    end_accuracy = sum(end_accuracies) / len(end_accuracies)\n",
        "    accuracy = sum(batch_accuracies) / len(batch_accuracies)\n",
        "\n",
        "    return start_accuracy, end_accuracy, accuracy"
      ],
      "metadata": {
        "id": "D89ULbdJpFFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_accuracy, end_accuracy, accuracy = model_accuracy(model,\n",
        "                                                        test_dataset,\n",
        "                                                        hyperparameters['batch_size'])\n",
        "\n",
        "print()\n",
        "print(f'Start Accuracy: {start_accuracy}')\n",
        "print(f'End Accuracy: {end_accuracy}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEIR0_ZOuKoC",
        "outputId": "c7f882f9-be64-4cc4-8576-0ed7301805e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1269/1269 [01:38<00:00, 12.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start Accuracy: 0.6116247326487909\n",
            "End Accuracy: 0.6617696724180352\n",
            "Accuracy: 0.636697202533413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}